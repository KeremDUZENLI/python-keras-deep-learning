{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b604394-f123-4e7e-9214-029583e92165",
   "metadata": {},
   "source": [
    "# Step 1: Downloading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2b4a6ef-3f62-4780-a5ba-3c77dc7f1cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path as op\n",
    "import tarfile\n",
    "from urllib.request import urlretrieve\n",
    "import xml.etree.ElementTree as etree\n",
    "import os\n",
    "from imageio import imread\n",
    "from skimage.transform import resize\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.layers import Input, Convolution2D, Dropout, GlobalAveragePooling2D, Flatten, Dense\n",
    "from keras.models import Model\n",
    "import h5py\n",
    "\n",
    "# Download Pascal VOC 2007 dataset\n",
    "URL_VOC = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\"\n",
    "FILE_VOC = \"Files/VOCtrainval_06-Nov-2007.tar\"\n",
    "FOLDER_VOC = \"Files/VOCdevkit\"\n",
    "\n",
    "if not op.exists(FILE_VOC):\n",
    "    print(f'Downloading from {URL_VOC} to {FILE_VOC}...')\n",
    "    urlretrieve(URL_VOC, FILE_VOC)\n",
    "\n",
    "if not op.exists(FOLDER_VOC):\n",
    "    print(f'Extracting {FILE_VOC}...')\n",
    "    tar = tarfile.open(FILE_VOC)\n",
    "    tar.extractall(path=\"Files\")\n",
    "    tar.close()\n",
    "\n",
    "# Download precomputed representations\n",
    "URL_REPRESENTATIONS = (\"https://github.com/m2dsupsdlclass/lectures-labs/\"\n",
    "                       \"releases/download/0.2/voc_representations.h5\")\n",
    "FILE_REPRESENTATIONS = \"Files/voc_representations.h5\"\n",
    "\n",
    "if not op.exists(FILE_REPRESENTATIONS):\n",
    "    print(f'Downloading from {URL_REPRESENTATIONS} to {FILE_REPRESENTATIONS}...')\n",
    "    urlretrieve(URL_REPRESENTATIONS, FILE_REPRESENTATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b4ebf3-6adb-49f6-b140-270cf8399ca1",
   "metadata": {},
   "source": [
    "# Step 2: Loading and Preprocessing Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "062b5add-b465-43ee-b941-9752253ebc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index to label mapping: {0: 'dog', 1: 'cat', 2: 'bus', 3: 'car', 4: 'aeroplane'}\n",
      "Label to index mapping: {'dog': 0, 'cat': 1, 'bus': 2, 'car': 3, 'aeroplane': 4}\n",
      "Number of images with annotations: 1264\n",
      "Sample annotation: {'size': (500, 333), 'filename': '000007.jpg', 'class': 'car', 'bbox': [141, 50, 500, 330]}\n"
     ]
    }
   ],
   "source": [
    "# Define classes and mappings\n",
    "filters = [\"dog\", \"cat\", \"bus\", \"car\", \"aeroplane\"]\n",
    "idx2labels = {k: v for k, v in enumerate(filters)}\n",
    "labels2idx = {v: k for k, v in idx2labels.items()}\n",
    "print(\"Index to label mapping:\", idx2labels)\n",
    "print(\"Label to index mapping:\", labels2idx)\n",
    "\n",
    "# Function to parse XML annotations\n",
    "def extract_xml_annotation(filename):\n",
    "    z = etree.parse(filename)\n",
    "    objects = z.findall(\"./object\")\n",
    "    size = (int(z.find(\".//width\").text), int(z.find(\".//height\").text))\n",
    "    fname = z.find(\"./filename\").text\n",
    "    dicts = [{obj.find(\"name\").text: [int(obj.find(\"bndbox/xmin\").text),\n",
    "                                     int(obj.find(\"bndbox/ymin\").text),\n",
    "                                     int(obj.find(\"bndbox/xmax\").text),\n",
    "                                     int(obj.find(\"bndbox/ymax\").text)]}\n",
    "             for obj in objects]\n",
    "    return {\"size\": size, \"filename\": fname, \"objects\": dicts}\n",
    "\n",
    "# Load and filter annotations\n",
    "annotations = []\n",
    "annotation_folder = \"Files/VOCdevkit/VOC2007/Annotations/\"\n",
    "for filename in sorted(os.listdir(annotation_folder)):\n",
    "    annotation = extract_xml_annotation(op.join(annotation_folder, filename))\n",
    "    new_objects = [obj for obj in annotation[\"objects\"] if list(obj.keys())[0] in filters]\n",
    "    if len(new_objects) == 1:  # Keep images with exactly one object\n",
    "        annotation[\"class\"] = list(new_objects[0].keys())[0]\n",
    "        annotation[\"bbox\"] = list(new_objects[0].values())[0]\n",
    "        annotation.pop(\"objects\")\n",
    "        annotations.append(annotation)\n",
    "\n",
    "print(\"Number of images with annotations:\", len(annotations))\n",
    "print(\"Sample annotation:\", annotations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9012799-9e6a-463e-b3b3-fe6b9a363766",
   "metadata": {},
   "source": [
    "# Step 3: Precomputing Image Representations with ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df9e6bc-eded-46c3-8767-29962e2c14f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load headless ResNet50\n",
    "base_model = ResNet50(include_top=False, weights='imagenet')\n",
    "headless_conv = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "print(\"Headless ResNet50 output shape:\", headless_conv.output_shape)\n",
    "\n",
    "# Function to predict on a batch of images\n",
    "def predict_batch(model, img_batch_path, img_size=(224, 224)):\n",
    "    img_list = []\n",
    "    for im_path in img_batch_path:\n",
    "        img = imread(im_path)\n",
    "        img = resize(img, img_size, preserve_range=True)\n",
    "        img = img.astype('float32')\n",
    "        img_list.append(img)\n",
    "    img_batch = np.stack(img_list, axis=0)\n",
    "    return model.predict(preprocess_input(img_batch))\n",
    "\n",
    "# Test on a small batch\n",
    "test_paths = [op.join(\"Files/VOCdevkit/VOC2007/JPEGImages\", annotations[i][\"filename\"]) for i in range(2)]\n",
    "test_reprs = predict_batch(headless_conv, test_paths)\n",
    "print(\"Test representations shape:\", test_reprs.shape)\n",
    "\n",
    "# Load precomputed representations\n",
    "with h5py.File('Files/voc_representations.h5', 'r') as h5f:\n",
    "    reprs = h5f['reprs'][:]\n",
    "print(\"Loaded representations shape:\", reprs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b72007-d98c-4448-8e15-0ccb39cf8481",
   "metadata": {},
   "source": [
    "# Step 4: Preparing Ground Truth Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b76ed-7d1a-4665-8716-3757ef63a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_resize = 224\n",
    "num_classes = len(labels2idx)\n",
    "\n",
    "def tensorize_ground_truth(annotations):\n",
    "    all_boxes = []\n",
    "    all_cls = []\n",
    "    for annotation in annotations:\n",
    "        cls = np.zeros(num_classes)\n",
    "        cls_idx = labels2idx[annotation[\"class\"]]\n",
    "        cls[cls_idx] = 1.0\n",
    "        coords = annotation[\"bbox\"]\n",
    "        size = annotation[\"size\"]\n",
    "        x1, y1, x2, y2 = (coords[0] * img_resize / size[0],\n",
    "                          coords[1] * img_resize / size[1],\n",
    "                          coords[2] * img_resize / size[0],\n",
    "                          coords[3] * img_resize / size[1])\n",
    "        cx, cy = (x2 + x1) / 2, (y2 + y1) / 2\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        boxes = np.array([cx, cy, w, h])\n",
    "        all_boxes.append(boxes)\n",
    "        all_cls.append(cls)\n",
    "    return np.vstack(all_cls), np.vstack(all_boxes)\n",
    "\n",
    "classes, boxes = tensorize_ground_truth(annotations)\n",
    "print(\"Classes shape:\", classes.shape)\n",
    "print(\"Boxes shape:\", boxes.shape)\n",
    "print(\"Sample classes:\", classes[:2])\n",
    "print(\"Sample boxes:\", boxes[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8f0bdd-f02b-4057-b15f-fd470b4ae48a",
   "metadata": {},
   "source": [
    "# Step 5: Interpreting Model Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50661db3-b20d-4288-886d-5c8901e37a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_output(cls, boxes, img_size=(500, 333)):\n",
    "    cls_idx = np.argmax(cls)\n",
    "    confidence = cls[cls_idx]\n",
    "    classname = idx2labels[cls_idx]\n",
    "    cx, cy, w, h = boxes\n",
    "    small_box = [max(0, cx - w / 2), max(0, cy - h / 2),\n",
    "                 min(img_resize, cx + w / 2), min(img_resize, cy + h / 2)]\n",
    "    fullsize_box = [int(small_box[0] * img_size[0] / img_resize),\n",
    "                    int(small_box[1] * img_size[1] / img_resize),\n",
    "                    int(small_box[2] * img_size[0] / img_resize),\n",
    "                    int(small_box[3] * img_size[1] / img_resize)]\n",
    "    return {\"class\": classname, \"confidence\": confidence, \"bbox\": fullsize_box}\n",
    "\n",
    "# Test interpretation\n",
    "img_idx = 1\n",
    "interpreted = interpret_output(classes[img_idx], boxes[img_idx], img_size=annotations[img_idx][\"size\"])\n",
    "print(\"Original annotation:\", annotations[img_idx])\n",
    "print(\"Interpreted output:\", interpreted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deed371-9478-4da9-a74c-794a20477ce0",
   "metadata": {},
   "source": [
    "# Step 6: Computing Intersection over Union (IoU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0053da-a370-4f0c-8c7b-f8d4aa21e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(boxA, boxB):\n",
    "    x0 = max(boxA[0], boxB[0])\n",
    "    y0 = max(boxA[1], boxB[1])\n",
    "    x1 = min(boxA[2], boxB[2])\n",
    "    y1 = min(boxA[3], boxB[3])\n",
    "    inter_area = max(x1 - x0, 0) * max(y1 - y0, 0) + 1\n",
    "    boxA_area = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxB_area = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    return inter_area / float(boxA_area + boxB_area - inter_area)\n",
    "\n",
    "# Test IoU\n",
    "img_idx = 1\n",
    "original = annotations[img_idx]\n",
    "interpreted = interpret_output(classes[img_idx], boxes[img_idx], img_size=original[\"size\"])\n",
    "print(\"IoU between original and interpreted:\", iou(original[\"bbox\"], interpreted[\"bbox\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad604f3f-5318-4b7f-92c7-123770f2603c",
   "metadata": {},
   "source": [
    "# Step 7: Building the Classification and Localization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d93c189-cd2f-40d6-932f-7ea1d24df91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classif_and_loc(num_classes):\n",
    "    model_input = Input(shape=(7, 7, 2048))\n",
    "    x = GlobalAveragePooling2D()(model_input)\n",
    "    x = Dropout(0.2)(x)\n",
    "    head_classes = Dense(num_classes, activation=\"softmax\", name=\"head_classes\")(x)\n",
    "    y = Convolution2D(4, (1, 1), strides=1, activation='relu', name='hidden_conv')(model_input)\n",
    "    y = Flatten()(y)\n",
    "    y = Dropout(0.2)(y)\n",
    "    head_boxes = Dense(4, name=\"head_boxes\")(y)\n",
    "    model = Model(model_input, outputs=[head_classes, head_boxes], name=\"resnet_loc\")\n",
    "    model.compile(optimizer=\"adam\", loss=['categorical_crossentropy', \"mse\"],\n",
    "                  loss_weights=[1., 1/(224*224)])\n",
    "    return model\n",
    "\n",
    "model = classif_and_loc(num_classes)\n",
    "model.summary()\n",
    "\n",
    "# Test prediction\n",
    "inputs = reprs[:10]\n",
    "out = model.predict(inputs)\n",
    "print(\"Model output shapes:\", out[0].shape, out[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703cf6e6-de10-44ad-b8d4-555ef14ad2b5",
   "metadata": {},
   "source": [
    "# Step 8: Visualizing Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8285614-63ca-4d33-88c4-3661019c730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch(axis, bbox, display_txt, color):\n",
    "    coords = (bbox[0], bbox[1]), bbox[2]-bbox[0]+1, bbox[3]-bbox[1]+1\n",
    "    axis.add_patch(plt.Rectangle(*coords, fill=False, edgecolor=color, linewidth=2))\n",
    "    axis.text(bbox[0], bbox[1], display_txt, bbox={'facecolor': color, 'alpha': 0.5})\n",
    "\n",
    "def plot_annotations(img_path, annotation=None, ground_truth=None):\n",
    "    img = imread(img_path)\n",
    "    plt.imshow(img)\n",
    "    current_axis = plt.gca()\n",
    "    if ground_truth:\n",
    "        text = \"gt \" + ground_truth[\"class\"]\n",
    "        patch(current_axis, ground_truth[\"bbox\"], text, \"red\")\n",
    "    if annotation:\n",
    "        conf = f\"{annotation['confidence']:.2f} \"\n",
    "        text = conf + annotation[\"class\"]\n",
    "        patch(current_axis, annotation[\"bbox\"], text, \"blue\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def display(index, ground_truth=True):\n",
    "    res = model.predict(reprs[index][np.newaxis])\n",
    "    output = interpret_output(res[0][0], res[1][0], img_size=annotations[index][\"size\"])\n",
    "    plot_annotations(op.join(\"Files/VOCdevkit/VOC2007/JPEGImages\", annotations[index][\"filename\"]),\n",
    "                     output, annotations[index] if ground_truth else None)\n",
    "\n",
    "# Test visualization\n",
    "display(2)\n",
    "display(194)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afac397d-2e2c-4e58-b45e-2c81295b8b85",
   "metadata": {},
   "source": [
    "# Step 9: Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876b7bc7-7518-423c-8888-8d84f5fda328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_and_iou(preds, trues, threshold=0.5):\n",
    "    sum_valid, sum_accurate, sum_iou = 0, 0, 0\n",
    "    num = len(preds)\n",
    "    for pred, true in zip(preds, trues):\n",
    "        iou_value = iou(pred[\"bbox\"], true[\"bbox\"])\n",
    "        if pred[\"class\"] == true[\"class\"] and iou_value > threshold:\n",
    "            sum_valid += 1\n",
    "        sum_iou += iou_value\n",
    "        if pred[\"class\"] == true[\"class\"]:\n",
    "            sum_accurate += 1\n",
    "    return sum_accurate / num, sum_iou / num, sum_valid / num\n",
    "\n",
    "def compute_acc(train=True):\n",
    "    if train:\n",
    "        beg, end = 0, (9 * len(annotations) // 10)\n",
    "        split_name = \"train\"\n",
    "    else:\n",
    "        beg, end = (9 * len(annotations)) // 10, len(annotations)\n",
    "        split_name = \"test\"\n",
    "    res = model.predict(reprs[beg:end])\n",
    "    outputs = [interpret_output(cls, box, img_size=annotations[i][\"size\"])\n",
    "               for i, (cls, box) in enumerate(zip(res[0], res[1]))]\n",
    "    acc, iou, valid = accuracy_and_iou(outputs, annotations[beg:end], threshold=0.5)\n",
    "    print(f'{split_name} acc: {acc:.3f}, mean iou: {iou:.3f}, acc_valid: {valid:.3f}')\n",
    "\n",
    "# Evaluate untrained model\n",
    "compute_acc(train=True)\n",
    "compute_acc(train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f4a4df-bbb2-4e36-a6be-87d5b1fea7b9",
   "metadata": {},
   "source": [
    "# Step 10: Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0c32b9-29e0-4022-a022-92f154d53849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "test_num = reprs.shape[0] // 10\n",
    "train_num = reprs.shape[0] - test_num\n",
    "train_inputs = reprs[:train_num]\n",
    "test_inputs = reprs[train_num:]\n",
    "train_cls, train_boxes = classes[:train_num], boxes[:train_num]\n",
    "test_cls, test_boxes = classes[train_num:], boxes[train_num:]\n",
    "print(\"Training samples:\", train_num)\n",
    "\n",
    "# Rebuild and train the model\n",
    "model = classif_and_loc(num_classes)\n",
    "batch_size = 32\n",
    "history = model.fit(train_inputs, y=[train_cls, train_boxes],\n",
    "                    validation_data=(test_inputs, [test_cls, test_boxes]),\n",
    "                    batch_size=batch_size, epochs=10, verbose=2)\n",
    "\n",
    "# Plot training history\n",
    "plt.plot(np.log(history.history[\"head_boxes_loss\"]), label=\"boxes_loss\")\n",
    "plt.plot(np.log(history.history[\"head_classes_loss\"]), label=\"classes_loss\")\n",
    "plt.plot(np.log(history.history[\"loss\"]), label=\"loss\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Log Loss\")\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.show()\n",
    "\n",
    "# Evaluate trained model\n",
    "compute_acc(train=True)\n",
    "compute_acc(train=False)\n",
    "\n",
    "# Visualize a test example\n",
    "display(194)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
